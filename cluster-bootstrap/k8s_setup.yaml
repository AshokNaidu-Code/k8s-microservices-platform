---
- name: 01 - Install Prerequisites (Docker, Kube Tools, Configure Cgroup)
  hosts: all
  become: yes
  gather_facts: yes
  vars:
    pod_network_cidr: 192.168.0.0/16
  tasks:
    - name: "A.0. CRITICAL WAIT: Wait for SSH Connection to be stable on all nodes"
      ansible.builtin.wait_for_connection:
        timeout: 300

    - name: Set SSH extra args for resiliency
      set_fact:
        ansible_ssh_extra_args: -o StrictHostKeyChecking=no -o ServerAliveInterval=50 -o ServerAliveCountMax=10

    - name: A.1. CRITICAL FIX - Ensure DNS is working by using Google Public DNS
      ansible.builtin.blockinfile:
        path: /etc/resolv.conf
        block: |
          nameserver 8.8.8.8
          nameserver 8.8.4.4
        create: yes
        marker: "# {mark} ANSIBLE MANAGED BLOCK - CRITICAL DNS FIX"
      when: inventory_hostname in groups['control_plane'] or inventory_hostname in groups['workers']

    - name: A.2. Update apt cache
      ansible.builtin.apt:
        update_cache: yes
        cache_valid_time: 3600

    - name: A.3. CRITICAL FIX - Ensure UFW (Uncomplicated Firewall) is disabled
      ansible.builtin.systemd:
        name: ufw
        state: stopped
        enabled: false
      ignore_errors: true

    - name: B1. Install Docker, containerd, Runc, and python3-pip
      ansible.builtin.apt:
        name:
          - docker.io
          - containerd
          - runc
          - apt-transport-https
          - ca-certificates
          - python3-pip
        state: present

    - name: B2. Ensure Docker is running and enabled
      systemd:
        name: docker
        state: started
        enabled: yes

    - name: C1. Ensure /etc/apt/keyrings directory exists
      ansible.builtin.file:
        path: /etc/apt/keyrings
        state: directory
        mode: "0755"

    - name: C2. Download Kubernetes GPG key to localhost and de-armor it
      ansible.builtin.shell: >
        curl -fsSL
        https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | gpg --dearmor
        -o /tmp/kubernetes-apt-keyring.gpg
      delegate_to: localhost
      run_once: yes
      become: no

    - name: C3. Copy downloaded GPG key from localhost to all target hosts
      ansible.builtin.copy:
        src: /tmp/kubernetes-apt-keyring.gpg
        dest: /etc/apt/keyrings/kubernetes-apt-keyring.gpg
        remote_src: no
        mode: "0644"

    - name: C4. Add Kubernetes repository to sources list
      ansible.builtin.apt_repository:
        repo: deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /
        state: present
        update_cache: yes
        filename: kubernetes

    - name: C5. Install kubelet, kubeadm, and kubectl
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present

    - name: C6. Hold kubelet, kubeadm, and kubectl to prevent auto-updates
      ansible.builtin.dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl

    - name: D0. CRITICAL FIX Disable and remove all swap (Required by Kubelet)
      ansible.builtin.shell: |
        swapoff -a
        sed -i '/\sswap\s/ s/^\(.*\)$/#\1/g' /etc/fstab
      changed_when: true

    - name: D1. Ensure Kubernetes prerequisites are met
      copy:
        content: |
          net.bridge.bridge-nf-call-iptables = 1
          net.ipv4.ip_forward = 1
        dest: /etc/sysctl.d/99-kubernetes-cri.conf
        mode: "0644"

    - name: D.2. Load br_netfilter and overlay modules
      community.general.modprobe:
        name: "{{ item }}"
        state: present
      with_items:
        - br_netfilter
        - overlay

    - name: D3. Apply sysctl parameters
      sysctl:
        name: "{{ item }}"
        value: "1"
        state: present
        reload: yes
      with_items:
        - net.bridge.bridge-nf-call-iptables
        - net.bridge.bridge-nf-call-ip6tables
        - net.ipv4.ip_forward

    - name: D4. Configure docker cgroup driver to systemd
      ansible.builtin.copy:
        content: |
          {
            "exec-opts": ["native.cgroupdriver=systemd"],
            "log-driver": "json-file",
            "log-opts": {
              "max-size": "100m"
            },
            "storage-driver": "overlay2"
          }
        dest: /etc/docker/daemon.json
        mode: "0644"
      notify: Restart docker

    - name: D5. Enable and restart kubelet to apply cgroup driver
      ansible.builtin.systemd:
        name: kubelet
        state: restarted
        enabled: yes

  handlers:
    - name: Restart docker
      ansible.builtin.systemd:
        name: docker
        state: restarted

- name: 02 - Initialize Control Plane
  hosts: control_plane
  gather_facts: yes
  become: yes
  vars:
    pod_network_cidr: 10.0.0.0/16
  tasks:
    - name: A0.85. Stop kubelet with sudo
      ansible.builtin.systemd:
        name: kubelet
        state: stopped
      when: inventory_hostname in groups['control_plane']
      ignore_errors: yes

    - name: A0.9. Deep clean with sudo - Delete everything
      ansible.builtin.shell: |
        set -e
        sudo systemctl stop kubelet || true
        sudo systemctl stop containerd || true
        sleep 5
        sudo rm -rf /etc/kubernetes/
        sudo rm -rf /var/lib/etcd/
        sudo rm -rf /var/lib/kubelet/
        sudo rm -rf /etc/cni/net.d/
        sudo rm -rf /var/run/kubernetes/
        sudo mkdir -p /etc/kubernetes/
        sudo systemctl start containerd || true
        sleep 10
      when: inventory_hostname in groups['control_plane']
      ignore_errors: yes

    - name: A0.95. Full kubeadm reset
      ansible.builtin.shell: |
        sudo kubeadm reset -f --ignore-preflight-errors=all || true
        sleep 10
      when: inventory_hostname in groups['control_plane']
      ignore_errors: yes

    - name: A1. Run kubeadm init on the control plane (Skip addons to avoid timeouts)
      ansible.builtin.shell: |
        set -e
        PUBLIC_IP=$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4)
        echo "Control Plane IP: {{ ansible_default_ipv4.address }}"
        echo "Public IP: $PUBLIC_IP"
        sudo kubeadm init \
          --kubernetes-version v1.29.15 \
          --pod-network-cidr=10.0.0.0/16 \
          --apiserver-advertise-address={{ ansible_default_ipv4.address }} \
          --control-plane-endpoint={{ ansible_default_ipv4.address }}:6443 \
          --apiserver-cert-extra-sans={{ ansible_default_ipv4.address }},$PUBLIC_IP \
          --cri-socket=unix:///run/containerd/containerd.sock \
          --upload-certs \
          --skip-phases=addon/coredns,addon/kube-proxy
      when: inventory_hostname in groups['control_plane']
      register: init_result
      retries: 2
      delay: 30
      until: init_result.rc == 0 or 'already exists' in init_result.stderr

    - name: A1.5. Wait for API server to stabilize (30 seconds)
      ansible.builtin.wait_for:
        timeout: 30
      when: init_result is success

    - name: A1.6. Manually install CoreDNS addon
      ansible.builtin.shell: |
        kubectl apply -f https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed \
        | sed 's/VERSION/1.10.1/g' \
        | sed 's/RELEASE/v1.10.1/g' \
        | kubectl apply -f -
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: init_result is success
      ignore_errors: yes

    - name: A1.7. Manually install kube-proxy addon
      ansible.builtin.shell: |
        kubectl apply -f https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/kube-proxy-daemonset/kube-proxy-daemonset.yaml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: init_result is success
      ignore_errors: yes

    - name: A1.8. CRITICAL INSTALL Install Python Kubernetes client
      ansible.builtin.pip:
        name: kubernetes
        state: latest
        executable: pip3
      when: init_result is success
      retries: 3
      delay: 5

    - name: A1.9. CRITICAL WAIT Wait for API Server port (6443) to open
      ansible.builtin.wait_for:
        host: "{{ ansible_default_ipv4.address }}"
        port: 6443
        delay: 5
        timeout: 120
        state: started
      when: init_result is success

    - name: A1.10. Create .kube directory for the user
      ansible.builtin.file:
        path: /home/{{ ansible_user }}/.kube
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: "0755"
      when: init_result is success

    - name: A1.11. Copy admin.conf to user's .kube directory
      ansible.builtin.copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/{{ ansible_user }}/.kube/config
        remote_src: yes
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: "0644"
      when: init_result is success

    - name: A1.12. Wait for connection to stabilize
      ansible.builtin.wait_for_connection:
        timeout: 60
      when: init_result is success

    - name: A2. Wait for API Server Pod to be running
      ansible.builtin.shell: >
        kubectl get pod -n kube-system -l
        component=kube-apiserver -o
        jsonpath='{.items[0].status.containerStatuses[*].ready}' | grep -q
        'true'
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: apiserver_ready_check
      until: apiserver_ready_check.rc == 0
      retries: 30
      delay: 10
      changed_when: false
      when: init_result is success

    - name: A2.5. Deploy Calico CNI (Standard Manifest)
      ansible.builtin.shell:
        # This deploys the standard, all-in-one Calico manifest (v3.28.0)
        cmd: |
          kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml --validate=false
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: init_result is success
      ignore_errors: yes

    - name: A2.6. Wait for Calico and Control Plane Pods to become Ready
      # This uses 'kubectl wait'â€”better than using sleep
      ansible.builtin.shell:
        cmd: |
          # Wait up to 300s (5 minutes) for all core pods to be 'Ready'
          kubectl wait --for=condition=Ready pods --all --all-namespaces --timeout=300s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: init_result is success
      ignore_errors: yes

    - name: A2.7. Wait for all nodes to become Ready
      ansible.builtin.shell:
        cmd: |
          for i in {1..10}; do
          kubectl get nodes | grep -q 'NotReady' || break
          sleep 15
          done
          kubectl get nodes -o wide
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: init_result is success 
      ignore_errors: yes


    - name: D1. Generate and store the join command for worker nodes
      ansible.builtin.shell:
        cmd: |
          kubeadm token create --print-join-command --ttl 0
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: worker_join_command
      changed_when: false
      when: init_result is success

    - name: D2. Store the join command for the next play
      ansible.builtin.set_fact:
        k8s_join_command: "{{ worker_join_command.stdout }}"
      when: init_result is success

- name: 03 - Join Worker Nodes
  hosts: workers
  become: yes
  tasks:
    - name: A. Wait for Control Plane to finish initialization
      ansible.builtin.wait_for_connection:
        timeout: 300

    - name: B. Reset worker node if previously configured
      ansible.builtin.command: kubeadm reset -f
      changed_when: true
      ignore_errors: yes

    - name: C. Join the cluster with error handling
      ansible.builtin.shell: |
        {{ hostvars[groups['control_plane'][0]]['k8s_join_command'] }}
      changed_when: true
      when:
        - hostvars[groups['control_plane'][0]]['k8s_join_command'] is defined
      ignore_errors: yes
      retries: 2
      delay: 60
